{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NAM2021_workshop_astro-ts-physics-dl.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "Weo94mkvmO0T",
        "DBaMfYPpmaCh",
        "E0DL8A_8b1Zs"
      ],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMo+hyQs4EobyAsYAy0rQKH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mariomorvan/nam21-astro-ts-physics-dl/blob/main/NAM2021_workshop_astro_ts_physics_dl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQnD7JVWlYKY"
      },
      "source": [
        "# Modelling astrophysical time series with physics-based deep learning\n",
        "### NAM 2021: *Machine Learning Methods for Research in Astrophysics*\n",
        "- *author*: Mario Morvan\n",
        "- *contact*: mario.morvan.18@ucl.ac.uk "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zyuo7OMMkQko"
      },
      "source": [
        "# Imports\n",
        "import numbers\n",
        "import tqdm\n",
        "import numpy as np\n",
        "import matplotlib.pylab as plt\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Plotting params \n",
        "plt.rcParams[\"figure.figsize\"] = (12, 6)\n",
        "plt.rcParams[\"font.size\"] = 14\n",
        "\n",
        "# device-agostic notebook\n",
        "if torch.cuda.is_available():\n",
        "  device = 'cuda'\n",
        "  print('Device name used:', torch.cuda.get_device_name())\n",
        "else:\n",
        "  device = 'cpu'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Weo94mkvmO0T"
      },
      "source": [
        "## I) Modelling Astronomical Time Series with RNN\n",
        "\n",
        "This section aims at presenting and experimenting with simple tool: an LSTM architecture slightly tweaked for imputing missing values in dataset of time series, opening various applications for datasets of astronomical time series. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGSi0g8FdwKW"
      },
      "source": [
        "Let's first define a dummy dataset made of the (additive) composition of a random walk and a sine with random offset and period."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUFUCgaRmBI1"
      },
      "source": [
        "class DummyDataset(torch.utils.data.Dataset):\n",
        "  \"\"\"A simple torch dataset combining random walk and sine processes\"\"\"\n",
        "  def __init__(self, seq_length, size=100, seed=None):\n",
        "    \"\"\"Define a DummyDataset object\n",
        "    \n",
        "    Args:\n",
        "      seq_length: int\n",
        "        lenght of the time series to generate\n",
        "      size: int\n",
        "        number of samples for the dataset\n",
        "      seed: int\n",
        "        manual seed to define for reproducibility (default None)\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    self.seq_length = seq_length\n",
        "    self.size = size\n",
        "    if seed is not None:\n",
        "      torch.manual_seed(seed)\n",
        "    \n",
        "  def __len__(self):\n",
        "    return self.size\n",
        "    \n",
        "  def __getitem__(self, index):\n",
        "    sine = torch.sin(torch.linspace(0, 50 * torch.rand(1).item(), self.seq_length) \n",
        "                      + torch.rand(1).item()*np.pi)\n",
        "    random_walk = (torch.rand(self.seq_length) - 0.5).cumsum(0)/3\n",
        "    gaussian_noise = 0  # torch.randn(self.seq_length) / 10\n",
        "    out = (sine + random_walk + gaussian_noise).unsqueeze(-1)\n",
        "    return (out - out[:1].repeat(self.seq_length, 1)) / 2 / (out.max(0, keepdims=True)[0] - out.min(0, keepdims=True)[0])\n",
        "\n",
        "item = DummyDataset(300, size=100)[0]     \n",
        "plt.figure()   \n",
        "plt.plot(item)\n",
        "plt.xlabel('time index')\n",
        "plt.ylabel('value')\n",
        "plt.title('Random sample from our DummyDataset')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "730TzqxYpwCr"
      },
      "source": [
        "# create train a test dummy datasets\n",
        "seq_length = 200\n",
        "batch_size = 64\n",
        "\n",
        "dataset = DummyDataset(seq_length, size=256, seed=0)\n",
        "dataset_test = DummyDataset(seq_length, size=64, seed=1)\n",
        "\n",
        "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "loader_test = DataLoader(dataset_test, batch_size=len(dataset_test), shuffle=False)\n",
        "batch_test = next(iter(loader_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_t61utLRbalr"
      },
      "source": [
        "Now let's teach first a basic stacked LSTM to forecast the next step in this dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-LCnFfZpN1W"
      },
      "source": [
        "hidden_size = 64\n",
        "num_layers = 2\n",
        "model = nn.LSTM(input_size=1, hidden_size=hidden_size, num_layers=num_layers, \n",
        "                batch_first=True)\n",
        "optimiser = torch.optim.Adam(model.parameters(), lr=0.1)\n",
        "criterion = lambda y, pred: F.mse_loss(y, pred)\n",
        "\n",
        "def train_forecaster(model, loader, optimiser, criterion, \n",
        "                     epochs=1, loader_val=None):\n",
        "  \"\"\"Train a basic forecasting pytorch RNN\"\"\"\n",
        "  model.train()\n",
        "  losses = []\n",
        "\n",
        "  for epoch in tqdm.tqdm(range(1, 1+epochs)):\n",
        "    epoch_loss = 0\n",
        "    for x in loader:\n",
        "      optimiser.zero_grad()\n",
        "      with torch.enable_grad():\n",
        "        pred, _ = model(x)\n",
        "        loss = criterion(x[:, 1:], pred[:, :-1])   # x_{t+1} ~ f(x_t)\n",
        "      loss.backward()\n",
        "      optimiser.step()\n",
        "      epoch_loss += loss.item()\n",
        "    losses.append(epoch_loss / len(loader))\n",
        "  return losses  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hh6BkGniqTi8"
      },
      "source": [
        "losses = train_forecaster(model, loader, optimiser, criterion, epochs=30)\n",
        "plt.plot(losses)\n",
        "plt.yscale('log')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NrRDYqE22ovp"
      },
      "source": [
        "# pred = predict_forecaster(model, batch_test)\n",
        "model.eval()\n",
        "pred, (h_n, c_n) = model(batch_test)\n",
        "\n",
        "i = np.random.randint(len(pred))\n",
        "plt.title('Prediction example on test set')\n",
        "plt.xlabel('Time')\n",
        "plt.plot(batch_test[i,1:,0].T)\n",
        "plt.plot(pred[i,:-1,0].detach().T)\n",
        "plt.show()\n",
        "\n",
        "plt.scatter(batch_test[:, 1:,0], pred[:,:-1,0].detach(), s=5)\n",
        "plt.plot([batch_test.min().item(), batch_test.max().item()], \n",
        "         [batch_test.min().item(), batch_test.max().item()], color='red',)\n",
        "plt.ylabel('Test predictions')\n",
        "plt.xlabel('Test targets')\n",
        "plt.show()\n",
        "# plt.scatter(batch_test[:, 1:,0], batch_test[:,:-1,0].detach(), s=5)\n",
        "# plt.plot([batch_test.min().item(), batch_test.max().item()], \n",
        "#          [batch_test.min().item(), batch_test.max().item()], color='red',)\n",
        "# plt.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIynxP_eVpvT"
      },
      "source": [
        "### Applications:\n",
        "- forecasting\n",
        "- anomaly detection\n",
        "- encoding latent representation\n",
        "\n",
        "### Improvements and ideas to explore together:\n",
        "- loss\n",
        "- window predictions\n",
        "- dropout\n",
        "- visualisation of latent representation with TSNE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZShPHfUfKRqK"
      },
      "source": [
        "class LSTMI(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size=None, num_layers=1, dropout=0.):\n",
        "        \"\"\"Define an LSTM Imputer network\n",
        "\n",
        "\n",
        "        Args:\n",
        "          input_size: dimensionality of the input sequences\n",
        "          hidden_size: Number of units for the LSTM cells\n",
        "          output_size: dimensionality of the output sequences.\n",
        "                       If default (None) will be set as input_size.\n",
        "          num_layers: number of LSTM layers\n",
        "          dropout: dropout rate to apply after all-but-last layers\n",
        "        Returns:\n",
        "          pytorch module \n",
        "\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size if output_size is not None else input_size\n",
        "        self.num_layers = num_layers\n",
        "        self.dropout = float(dropout)\n",
        "\n",
        "        self.lstm_cells = nn.ModuleList([nn.LSTMCell(input_size=self.input_size, hidden_size=self.hidden_size)])\n",
        "        self.lstm_cells.extend([nn.LSTMCell(input_size=self.hidden_size, hidden_size=self.hidden_size)\n",
        "                                for _ in range(self.num_layers - 1)])\n",
        "        if dropout != 0:\n",
        "            self.dropout_layer = nn.Dropout(self.dropout)\n",
        "\n",
        "        self.fc = nn.Linear(self.hidden_size, self.output_size)\n",
        "\n",
        "        self.h_t = None\n",
        "        self.c_t = None\n",
        "        self.out_t = None\n",
        "\n",
        "    def init_state(self, batch_size, device=None, dtype=None):\n",
        "      \"\"\"Initialise the network's states\"\"\"\n",
        "      self.h_t = [torch.randn(batch_size, self.hidden_size, device=device, dtype=dtype)\n",
        "                  for _ in range(self.num_layers)]\n",
        "      self.c_t = [torch.randn(batch_size, self.hidden_size, device=device, dtype=dtype)\n",
        "                  for _ in range(self.num_layers)]\n",
        "      self.out_t = torch.randn(batch_size, self.output_size, device=device, dtype=dtype)\n",
        "\n",
        "    def init_state_like(self, x):\n",
        "      \"\"\"Initialise the network's on the model of a given input\"\"\"\n",
        "      self.init_state(len(x), x.device, x.dtype)\n",
        "\n",
        "    def forward(self, x, z=None, m=None, mask_nans=True):\n",
        "      \"\"\"Performs a forward pass\n",
        "\n",
        "      :param x: Input vector\n",
        "      :param m: Imputation mask - 1/True for keeping input and 0/False for forcing dynamic imputation\n",
        "      :return: Output vector with imputed values\n",
        "      \"\"\"\n",
        "\n",
        "      # Checking - casting inputs\n",
        "      if m is not None:\n",
        "        if x.shape != m.shape:\n",
        "          mess = f'x and m must have the same shape ({x.shape} != {m.shape})'\n",
        "          raise RuntimeError(mess)\n",
        "      else:\n",
        "        m = torch.ones_like(x)\n",
        "\n",
        "      if mask_nans:\n",
        "        m *= (x == x)\n",
        "\n",
        "      if len(x.shape) == 2:\n",
        "        warnings.warn('input has only 2 dims')\n",
        "        x = x.unsqueeze(-1)\n",
        "        m = m.unsqueeze(-1)\n",
        "      elif len(x.shape) != 3:\n",
        "        print(x.shape)\n",
        "        raise ValueError('wrong shape')\n",
        "\n",
        "      batch_size, len_seq, n_dim = x.shape\n",
        "\n",
        "      if z is not None:\n",
        "        zdim = z.shape[-1]\n",
        "      else:\n",
        "        zdim = 0\n",
        "      if n_dim + zdim != self.input_size:\n",
        "        mess = f'input (dim={n_dim}) and covariate (dim={zdim}) dimensions must' \\\n",
        "                + f'add to input_size param ({self.input_size})'\n",
        "        raise RuntimeError(mess)\n",
        "\n",
        "      m = m.type(x.dtype)\n",
        "      inverted_m = (torch.ones_like(m, device=x.device) - m)\n",
        "\n",
        "      # Preparing input\n",
        "      if z is not None:\n",
        "        input_masked = torch.cat((x * m, z), dim=-1)\n",
        "      else:\n",
        "        input_masked = x * m\n",
        "      outputs = []\n",
        "\n",
        "      for t in range(len_seq):\n",
        "        pred_t = self.out_t\n",
        "        input_t = input_masked[:, t] + F.pad(pred_t * inverted_m[:, t], (0, zdim))\n",
        "        for k, lstm_cell in enumerate(self.lstm_cells):\n",
        "          if k == 0:\n",
        "            self.h_t[0], self.c_t[0] = lstm_cell(input_t, (self.h_t[0], self.c_t[0]))\n",
        "          else:\n",
        "            self.h_t[k], self.c_t[k] = lstm_cell(self.h_t[k - 1], (self.h_t[k], self.c_t[k]))\n",
        "          if self.dropout != 0 and k < self.num_layers - 1:\n",
        "            self.h_t[k] = self.dropout_layer(self.h_t[k])\n",
        "            self.c_t[k] = self.dropout_layer(self.c_t[k])\n",
        "        self.out_t = self.fc(self.h_t[-1])\n",
        "        outputs.append(self.out_t.unsqueeze(1))\n",
        "      return torch.cat(outputs, 1)\n",
        "\n",
        "\n",
        "def train_lstmi_batch(model, loader, optimiser, criterion, epochs=1):\n",
        "  \"\"\"Train a LSTMI module\"\"\"\n",
        "  model.train()\n",
        "  losses = []\n",
        "\n",
        "  for epoch in tqdm.tqdm(range(1, 1+epochs)):\n",
        "    epoch_loss = 0\n",
        "    for x in loader:\n",
        "      optimiser.zero_grad()\n",
        "      model.init_state_like(x)\n",
        "      with torch.enable_grad():\n",
        "        pred = model(x)\n",
        "        loss = criterion(x[:, 1:], pred[:, :-1])   # x_{t+1} ~ f(x_t)\n",
        "      loss.backward()\n",
        "      optimiser.step()\n",
        "      epoch_loss += loss.item()\n",
        "    losses.append(epoch_loss / len(loader))\n",
        "  return losses  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5eApdMmPlHJ"
      },
      "source": [
        "model = LSTMI(1, hidden_size=hidden_size, num_layers=num_layers)\n",
        "optimiser = torch.optim.Adam(model.parameters(), lr=0.1)\n",
        "criterion = lambda y, pred: F.mse_loss(y, pred)\n",
        "losses = train_lstmi_batch(model, loader, optimiser, criterion, epochs=200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7SdrQTegOlo"
      },
      "source": [
        "# learning curve\n",
        "plt.plot(losses)\n",
        "plt.yscale('log')\n",
        "plt.title('learning curve')\n",
        "plt.xlabel('Training loss')\n",
        "plt.ylabel('MSE')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UG750-cGPND5"
      },
      "source": [
        "# Check of predictions bias\n",
        "model.eval()\n",
        "model.init_state_like(batch_test)\n",
        "pred = model(batch_test)\n",
        "\n",
        "i = np.random.randint(len(pred))\n",
        "plt.plot(batch_test[i,1:,0].T)\n",
        "plt.plot(pred[i,:-1,0].detach().T)\n",
        "plt.show()\n",
        "\n",
        "plt.scatter(batch_test[:, 1:,0], pred[:,:-1,0].detach(), s=5)\n",
        "plt.plot([batch_test.min().item(), batch_test.max().item()], \n",
        "         [batch_test.min().item(), batch_test.max().item()], color='red')\n",
        "plt.ylabel('Test predictions')\n",
        "plt.xlabel('Test targets')\n",
        "plt.show()\n",
        "plt.show()\n",
        "# plt.scatter(batch_test[:, 1:,0], batch_test[:,:-1,0].detach(), s=5)\n",
        "# plt.plot([batch_test.min().item(), batch_test.max().item()], \n",
        "#          [batch_test.min().item(), batch_test.max().item()], color='red',)\n",
        "# plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2N6zbrzTT1E"
      },
      "source": [
        "### Applications\n",
        "- Anomaly detections\n",
        "- Imputing\n",
        "- Modelling and fiting gaps\n",
        "- More flexible learning (include in the training loss)\n",
        "\n",
        "\n",
        "### Suggestions:\n",
        "- Gaussian Loss \n",
        "- Gap imputing metric\n",
        "- See improvements with growing dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBaMfYPpmaCh"
      },
      "source": [
        "## II) Embed differentiable physics model in DL framework\n",
        "\n",
        "Why hard-coding the physics model in a DL framework? \n",
        "- Computational efficiency, with automatic differenciation and GPU acceleration\n",
        "- Combine with NNs\n",
        "\n",
        "\n",
        "Physics model requirements:\n",
        "- implemented in DL framework (here Pytorch)\n",
        "- vectorised to process batches of samples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dP_WuT6YLixJ"
      },
      "source": [
        "Let's define a simple flare model in Pytorch: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ow4-HpV3QyT-"
      },
      "source": [
        "# Defining physics model and dataset class\n",
        "\n",
        "# def physics_model():\n",
        "\n",
        "def compute_flare(time, a_0, tau_g, tau_e, t_0=50):\n",
        "  \"ref: https://academic.oup.com/mnras/article/445/3/2268/2907951\"\n",
        "  out = torch.empty_like(time)\n",
        "  out[time <= t_0] = a_0 * torch.exp(-(time[time<=t_0] - t_0)**2 / (2*tau_g**2))\n",
        "  out[time > t_0] = a_0 * torch.exp(-(time[time>t_0] - t_0)**2 / (tau_e**2))\n",
        "  return out\n",
        "\n",
        "def compute_flare_batch(time, a_0, tau_g, tau_e, t_0=50):\n",
        "  \"\"\" \n",
        "  \"\"\"\n",
        "  batch_size = len(a_0)\n",
        "  time = time[None,:].repeat(batch_size, 1)\n",
        "  a_0 = a_0.reshape(batch_size,1)\n",
        "  tau_g = tau_g.reshape(batch_size,1)\n",
        "  tau_e = tau_e.reshape(batch_size,1)\n",
        "  # t_0 = t_0.reshape(batch_size,1)\n",
        "\n",
        "  # temp_g = (time - t_0) / 2*tau_g\n",
        "  # temp_e =  / tau_e\n",
        "\n",
        "  return a_0 * torch.exp(-((time - t_0) * ((time < t_0) / 2*tau_g / tau_e))**2)\n",
        "\n",
        "\n",
        "class PhysicsDataset():\n",
        "  \"\"\"\n",
        "\n",
        "  \"\"\"\n",
        "  def __init__(self, physics_model, bounds, target_params, \n",
        "               seq_length=200, size=100, noise_level=1e-4, seed=0):\n",
        "    super().__init__()\n",
        "    self.physics_model = physics_model\n",
        "    self.bounds = bounds\n",
        "    self.target_params = target_params\n",
        "    self.seq_length = seq_length\n",
        "    self.size = size\n",
        "    self.noise_level = noise_level\n",
        "    if seed is not None:\n",
        "        torch.manual_seed(seed)\n",
        "    self.device = device\n",
        "    \n",
        "    self.params = dict()\n",
        "    self._sample_priors(self.bounds)\n",
        "    self.time = torch.linspace(0, 100,self.seq_length, device=device)\n",
        "    self.noise = torch.randn(self.size, self.seq_length, device=device) * self.noise_level\n",
        "\n",
        "  def _sample_priors(self, bounds):\n",
        "    for par in bounds:\n",
        "      self.params[par] = torch.rand(self.size, device=self.device) * (bounds[par][1] - bounds[par][0]) + bounds[par][0]\n",
        "\n",
        "  def _get_item_params(self, index):\n",
        "    return {par: self.params[par][index] for par in self.params}\n",
        "      \n",
        "  def __len__(self):\n",
        "      return self.size\n",
        "  \n",
        "  def __getitem__(self, index):\n",
        "    x =  (self.physics_model(self.time, **self._get_item_params(index)) \n",
        "          + self.noise[index])\n",
        "    target = torch.tensor([self._get_item_params(index)[par] for par in self.target_params])\n",
        "    # additional_params = torch.tensor([self._get_item_params(index)[par] for par in self.bounds if par not in self.target_params])\n",
        "    return x.to(self.device), target.to(self.device) #, additional_params\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-C8NOnZ4C3Z5"
      },
      "source": [
        "# plot one example\n",
        "bounds = {'a_0': [1, 5], 'tau_g':[4, 10], 'tau_e':[1, 5], 't_0':[50,50]}\n",
        "target_params = ['a_0', 'tau_g', 'tau_e']\n",
        "dataset = PhysicsDataset(compute_flare, bounds, target_params, noise_level=0.05)\n",
        "\n",
        "x, target = dataset[0]\n",
        "\n",
        "plt.plot(x.detach().cpu())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T96rmi071xcs"
      },
      "source": [
        "# Reproducibility - does not seem necessary or compatible with some cuda functions\n",
        "torch.use_deterministic_algorithms(False)\n",
        "\n",
        "def seed_worker(worker_id):\n",
        "    worker_seed = torch.initial_seed() % 2**32\n",
        "    numpy.random.seed(worker_seed)\n",
        "    random.seed(worker_seed)\n",
        "# g = torch.Generator()\n",
        "# g.manual_seed(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8jqEboqzzX5"
      },
      "source": [
        "# Instantiating datasets and loaders\n",
        "\n",
        "##### SIMPLIFY CELL\n",
        "\n",
        "# General params\n",
        "target_params = ['a_0', 'tau_g', 'tau_e']\n",
        "seq_length = 100\n",
        "size_train = 512\n",
        "batch_size = 128\n",
        "size_val = 512\n",
        "size_test = 1024\n",
        "\n",
        "device = 'cuda'\n",
        "\n",
        "# Train dataset\n",
        "bounds_train = {'a_0': [1, 2], 'tau_g':[4, 6], 'tau_e':[2, 5]}\n",
        "bounds_val_2 = {'a_0': [0, 3], 'tau_g':[3, 8], 'tau_e':[1, 6]}\n",
        "noise_train = 0.01\n",
        "\n",
        "dataset_train = PhysicsDataset(compute_flare, bounds_train, target_params, \n",
        "                               seq_length=seq_length, size=size_train, \n",
        "                               noise_level=noise_train, seed=0)\n",
        "\n",
        "loader_train = DataLoader(dataset_train, batch_size=batch_size, \n",
        "                          shuffle=True, \n",
        "                          # num_workers=1,                            #!: investigate why this fails\n",
        "                          # worker_init_fn=seed_worker, generator=g   #!: does not seem necessary for reproducibility\n",
        "                          )\n",
        "\n",
        "# Val dataset 1 (same parameter space as training)\n",
        "dataset_val_1 = PhysicsDataset(compute_flare, bounds_train, target_params, \n",
        "                                seq_length=seq_length, size=size_val, \n",
        "                                noise_level=noise_train, seed=1)\n",
        "loader_val_1 = DataLoader(dataset_val_1, batch_size=len(dataset_val_1))\n",
        "batch_val_1 = next(iter(loader_val_1))\n",
        "\n",
        "# Test dataset 1 (same parameter space as training)\n",
        "dataset_test_1 = PhysicsDataset(compute_flare, bounds_train, target_params, \n",
        "                                seq_length=seq_length, size=size_test, \n",
        "                                noise_level=noise_train, seed=4)\n",
        "loader_test_1 = DataLoader(dataset_test_1, batch_size=len(dataset_test_1))\n",
        "x_test_1, target_test_1 = next(iter(loader_test_1))\n",
        "\n",
        "\n",
        "# Val dataset 2 (different parameter space)\n",
        "dataset_val_2 = PhysicsDataset(compute_flare, bounds_val_2, target_params, \n",
        "                                seq_length=seq_length, size=size_val, \n",
        "                                noise_level=noise_train, seed=2)\n",
        "loader_val_2 = DataLoader(dataset_val_2, batch_size=len(dataset_val_2))\n",
        "batch_val_2 = next(iter(loader_val_2))\n",
        "\n",
        "\n",
        "# Test dataset 2 (different parameter space)\n",
        "dataset_test_2 = PhysicsDataset(compute_flare, bounds_val_2, target_params, \n",
        "                                seq_length=seq_length, size=size_test, \n",
        "                                noise_level=noise_train, seed=5)\n",
        "loader_test_2 = DataLoader(dataset_test_2, batch_size=len(dataset_test_2))\n",
        "x_test_2, target_test_2 = next(iter(loader_test_2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJIaKhvFnr9i"
      },
      "source": [
        "# compute_flare_batch(dataset_test_1.time, **dataset_test_1.params)\n",
        "# [compute_flare(dataset_train.time, **{par: dataset_train.params[par][i:i+1] for par in dataset_train.params}) for i in range(512)]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GbyQq4u93fou"
      },
      "source": [
        "# plotting some signals"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGA85RQ9IxTa"
      },
      "source": [
        "# A simple network to solve the inverse problem data -> params\n",
        "\n",
        "class Network(nn.Module):\n",
        "  \"\"\"Define a simple MLP in pytorch\"\"\"\n",
        "  def __init__(self, seq_length, out_dim):\n",
        "    super().__init__()\n",
        "    self.fc1 = nn.Linear(seq_length, 256)\n",
        "    self.fc2 = nn.Linear(256, 64)\n",
        "    self.fc3 = nn.Linear(64, 32)\n",
        "    self.fc4 = nn.Linear(32, out_dim)\n",
        "      \n",
        "  def forward(self, x):\n",
        "    out = F.relu(self.fc1(x))\n",
        "    out = F.relu(self.fc2(out))\n",
        "    out = F.relu(self.fc3(out))\n",
        "    out = self.fc4(out)\n",
        "    return out\n",
        "\n",
        "\n",
        "def train_network(model, loader, optimiser, criterion,\n",
        "                  epochs=1, batch_val={}, metric_val=None, eval_epochs=[]):\n",
        "  \"\"\"Train a pytorch module\"\"\"\n",
        "  losses = {'train':[]}\n",
        "  losses.update({name: [] for name in batch_val})\n",
        "\n",
        "  for epoch in tqdm.tqdm(range(1, 1+epochs)):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    for x, target in loader:\n",
        "      optimiser.zero_grad()\n",
        "      x = x.to(device)\n",
        "      target = target.to(device)\n",
        "      with torch.enable_grad():\n",
        "        pred = model(x)\n",
        "        loss = criterion(x, target, pred)\n",
        "      loss.backward()\n",
        "      optimiser.step()\n",
        "      epoch_loss += loss.item()\n",
        "    losses['train'].append(epoch_loss / len(loader))\n",
        "    if metric_val is not None and batch_val is not None and epoch in eval_epochs:\n",
        "      model.eval()\n",
        "      for name, batch in batch_val.items():\n",
        "        x_val, target_val = batch[0].to(device), batch[1].to(device)\n",
        "        pred_val = model(x_val)\n",
        "        losses[name].append(metric_val(x_val, target_val, pred_val))\n",
        "  return losses  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cr2p1DC05lfJ"
      },
      "source": [
        "# defining two losses\n",
        "\n",
        "def naive_loss(x, target, pred):\n",
        "  \"\"\"Wrapper around pytorch mse_loss to add inputs to signature\n",
        "\n",
        "  Args: \n",
        "    x: torch.Tensor \n",
        "      input time series of shape (batch_size, T) or (T,). (unused argument)\n",
        "    target: torch.Tensor\n",
        "      output targets of shape (batch_size, dim) or (dim,)\n",
        "    pred: torch.Tensor\n",
        "      predicted targets of shape (batch_size, dim) or (dim,)\n",
        "  Return: torch.Tensor\n",
        "    mean squared error value between target and predictions\n",
        "  \"\"\"\n",
        "  return F.mse_loss(target, pred)\n",
        "\n",
        "def hybrid_loss(dataset, beta=1):\n",
        "  \"\"\"Define a hybrid regression & reconstruction loss function\n",
        "\n",
        "  Args:\n",
        "    dataset: PhysicsDataset\n",
        "      torch dataset with arguments target_params, time and physics_model\n",
        "    beta: \n",
        "      weight parameter between regression and reconstruction terms: \n",
        "      loss = regression + beta * reconstruction\n",
        "  Return: function\n",
        "    loss function associated with provided dataset and beta parameter\n",
        "  \"\"\"\n",
        "  def loss_function(x, target, pred):\n",
        "    \"\"\"Compute the hybrid loss\n",
        "\n",
        "    Args:\n",
        "\n",
        "\n",
        "    Return:\n",
        "      \n",
        "    \"\"\"\n",
        "    pred_dict = {dataset.target_params[i]: pred[:,i] for i in range(len(dataset.target_params))}  # Iterate over feature dimension to produce dict of outputs\n",
        "    reconstructed_time_series = compute_flare_batch(dataset.time.to(device), **pred_dict)\n",
        "    physics_reconstruction_term = F.mse_loss(x, reconstructed_time_series)\n",
        "    return naive_loss(x, target, pred) + beta * physics_reconstruction_term\n",
        "  return loss_function\n",
        "\n",
        "# Instance 2 scenarios\n",
        "scenarios = ['naive', 'hybrid']\n",
        "network = {scenario: Network(seq_length, len(target_params)).to(device) for scenario in scenarios}\n",
        "optimiser = {scenario: torch.optim.Adam(network[scenario].parameters(), lr=0.002) for scenario in scenarios}\n",
        "loss = {'naive': naive_loss,\n",
        "        'hybrid': hybrid_loss(dataset_train,beta=0.01)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gcPOai6_eiQA"
      },
      "source": [
        "# Running experiment with two losses\n",
        "loss_history = dict()\n",
        "epochs = 1500\n",
        "eval_epochs = list(range(1, 1+epochs, 10))\n",
        "for scenario in scenarios:\n",
        "  loss_history[scenario] = train_network(network[scenario], loader_train, optimiser[scenario], loss[scenario], epochs=epochs, \n",
        "                                         batch_val={'val_1': batch_val_1, 'val_2': batch_val_2}, metric_val=naive_loss, eval_epochs=eval_epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CnmlF_LWF6RY"
      },
      "source": [
        "# Plot learning curves\n",
        "plt.figure(figsize=(10,6))\n",
        "\n",
        "linestyle = {'naive': 'solid', 'hybrid': 'dotted'}\n",
        "alpha = {'naive': 0.7, 'hybrid': 1}\n",
        "for scenario in scenarios:\n",
        "  plt.plot(eval_epochs, loss_history[scenario]['val_1'], label=f'Val 1 ({scenario})', \n",
        "           c='blue', linestyle=linestyle[scenario], alpha=alpha[scenario])\n",
        "  plt.plot(eval_epochs, loss_history[scenario]['val_2'], label=f'Val 2 ({scenario})', \n",
        "           c='green', linestyle=linestyle[scenario], alpha=alpha[scenario])\n",
        "  # plt.plot(eval_epochs, loss_history[scenario]['val_3'], label=f'Val 3 ({scenario})', \n",
        "  #          c='red', linestyle=linestyle[scenario], alpha=alpha[scenario])\n",
        "plt.legend()\n",
        "plt.yscale('log')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('MSE')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7oTJj9Lbgqe"
      },
      "source": [
        "# evaluation on test sets\n",
        "\n",
        "scores_1 = dict()\n",
        "scores_2 = dict()\n",
        "\n",
        "for scenario in scenarios:\n",
        "  scores_1[scenario] = naive_loss(x_test_1, network[scenario](x_test_1), target_test_1).item()\n",
        "  scores_2[scenario] = naive_loss(x_test_2, network[scenario](x_test_2), target_test_2).item()\n",
        "print(f'scores_1: {scores_1}\\n', f'scores_2: {scores_2}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5pbx43yZvxa"
      },
      "source": [
        "Pros:\n",
        "- accuracy\n",
        "- stability\n",
        "- generalisability\n",
        "\n",
        "Cons:\n",
        "- need the physics model implemented in DL framework\n",
        "- physics model suceptible of adding complexity to the network's! \n",
        "- training may require further tuning to accomodate for different loss terms\n",
        "\n",
        "Improvements:\n",
        "- hyperoptimisation for the loss weight $\\beta$ \n",
        "- hyperoptim for LRs in both cases\n",
        "\n",
        "To go further:\n",
        "- use your own physics model! \n",
        "- design transfer learning and meta-learning experiments to assess generalisability"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0DL8A_8b1Zs"
      },
      "source": [
        "\n",
        "## III) Bonus: Combine RNN detrending wit differentiable physics model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bz8bZ0eCVcLh"
      },
      "source": [
        "Using the two different tools to perform denoising and fitting in the same time.\n",
        " - RNN to model and impute multiple time series with masked signal\n",
        " - physics model on the residuals\n",
        " - all trained end-to-end\n",
        "\n",
        "Inference echniques:\n",
        " - VAE\n",
        " - SVI \n",
        "\n",
        "Let's code an example below."
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}