{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NAM2021_workshop_astro-ts-physics-dl.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "Z2MbmzBjTu6M"
      ],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyM8Vh/NWM+3Zz5j1kWwlfPK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mariomorvan/nam21-astro-ts-physics-dl/blob/main/NAM2021_workshop_astro_ts_physics_dl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQnD7JVWlYKY"
      },
      "source": [
        "# Modelling astrophysical time series with physics-based deep learning\n",
        "---\n",
        "### NAM 2021: *Machine Learning Methods for Research in Astrophysics*\n",
        "- *author*: Mario Morvan\n",
        "- *collaborators*: Nikolaos Nikolaou, Angelos Tsiaras, Ingo Waldmann, Gordon Yip, ... & UCL exo group\n",
        "- *contact*: mario.morvan.18@ucl.ac.uk "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_X5XcFai33V"
      },
      "source": [
        "To run this notebook with a GPU, go to **Runime** > **change runtime type** and select **GPU**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zyuo7OMMkQko"
      },
      "source": [
        "# Imports\n",
        "import numbers\n",
        "import tqdm\n",
        "import numpy as np\n",
        "import matplotlib.pylab as plt\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# device-agostic notebook\n",
        "if torch.cuda.is_available():\n",
        "  device = 'cuda'\n",
        "  print('Device name used:', torch.cuda.get_device_name())\n",
        "else:\n",
        "  device = 'cpu'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bke3dS_FW6LY"
      },
      "source": [
        "# Matplotlib default params \n",
        "plt.rcParams[\"figure.figsize\"] = (12, 6)\n",
        "plt.rcParams[\"font.size\"] = 14"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Weo94mkvmO0T"
      },
      "source": [
        "## I) Modelling Astronomical Time Series with RNN\n",
        "\n",
        "This section aims at presenting and experimenting with simple tool: an LSTM architecture slightly tweaked for imputing missing values in dataset of time series, opening various applications for datasets of astronomical time series. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGSi0g8FdwKW"
      },
      "source": [
        "Let's first define a dummy dataset made of the (additive) composition of a random walk and a sine with random offset and period."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUFUCgaRmBI1"
      },
      "source": [
        "class DummyDataset(torch.utils.data.Dataset):\n",
        "  \"\"\"A simple torch dataset combining random walk and sine processes\"\"\"\n",
        "  def __init__(self, seq_length, size=100, seed=None):\n",
        "    \"\"\"Define a DummyDataset object\n",
        "    \n",
        "    Args:\n",
        "      seq_length: int\n",
        "        lenght of the time series to generate\n",
        "      size: int\n",
        "        number of samples for the dataset\n",
        "      seed: int\n",
        "        manual seed to define for reproducibility (default None)\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    self.seq_length = seq_length\n",
        "    self.size = size\n",
        "    if seed is not None:\n",
        "      torch.manual_seed(seed)\n",
        "    \n",
        "  def __len__(self):\n",
        "    return self.size\n",
        "    \n",
        "  def __getitem__(self, index):\n",
        "    sine = torch.sin(torch.linspace(0, 50 * torch.rand(1).item(), self.seq_length) \n",
        "                      + torch.rand(1).item()*np.pi)\n",
        "    random_walk = (torch.rand(self.seq_length) - 0.5).cumsum(0)/3\n",
        "    gaussian_noise = 0  # torch.randn(self.seq_length) / 10\n",
        "    out = (sine + random_walk + gaussian_noise).unsqueeze(-1)\n",
        "    #return (out - out.mean(0, keepdims=True)) / 2 / (out.std(0, keepdims=True)[0] - out.min(0, keepdims=True)[0])\n",
        "    return (out - out[:1].repeat(self.seq_length, 1)) / 2 / (out.max(0, keepdims=True)[0] - out.min(0, keepdims=True)[0]) + 0.5\n",
        "\n",
        "    return (out - out[:1].repeat(self.seq_length, 1)) / 2 / (out.max(0, keepdims=True)[0] - out.min(0, keepdims=True)[0])\n",
        "\n",
        "item = DummyDataset(300, size=100)[0]     \n",
        "plt.figure()   \n",
        "plt.plot(item)\n",
        "plt.xlabel('time index')\n",
        "plt.ylabel('value')\n",
        "plt.title('Random sample from our DummyDataset')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "730TzqxYpwCr"
      },
      "source": [
        "# Create train a test dummy datasets\n",
        "seq_length = 200\n",
        "batch_size = 64\n",
        "\n",
        "dataset = DummyDataset(seq_length, size=256, seed=0)\n",
        "dataset_test = DummyDataset(seq_length, size=64, seed=1)\n",
        "\n",
        "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "loader_test = DataLoader(dataset_test, batch_size=len(dataset_test), shuffle=False)\n",
        "batch_test = next(iter(loader_test)).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8TOdgVu03uN"
      },
      "source": [
        "#### Simple forecasting LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_t61utLRbalr"
      },
      "source": [
        "Now let's teach first a basic stacked LSTM to forecast the next step in this dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-LCnFfZpN1W"
      },
      "source": [
        "hidden_size = 32\n",
        "num_layers = 2\n",
        "\n",
        "model = nn.LSTM(input_size=1, hidden_size=hidden_size, num_layers=num_layers, \n",
        "                batch_first=True)\n",
        "\n",
        "\n",
        "model = model.to(device)\n",
        "optimiser = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "criterion = lambda y, pred: F.mse_loss(y, pred)\n",
        "\n",
        "def train_forecaster(model, loader, optimiser, criterion, epochs=1, device=None):\n",
        "  \"\"\"Train a basic forecasting pytorch RNN\"\"\"\n",
        "  model.train()\n",
        "  losses = []\n",
        "\n",
        "  for epoch in tqdm.tqdm(range(1, 1+epochs)):\n",
        "    epoch_loss = 0\n",
        "    for x in loader:\n",
        "      x = x.to(device)\n",
        "      optimiser.zero_grad()\n",
        "      with torch.enable_grad():\n",
        "        pred, _ = model(x)\n",
        "        loss = criterion(x[:, 1:], pred[:, :-1])   # x_{t+1} ~ f(x_t)\n",
        "      loss.backward()\n",
        "      optimiser.step()\n",
        "      epoch_loss += loss.item()\n",
        "    losses.append(epoch_loss / len(loader))\n",
        "  return losses  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hh6BkGniqTi8"
      },
      "source": [
        "losses = train_forecaster(model, loader, optimiser, criterion, epochs=200, \n",
        "                          device=device)\n",
        "plt.plot(losses)\n",
        "plt.yscale('log')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NrRDYqE22ovp"
      },
      "source": [
        "model.eval()\n",
        "pred, (h_n, c_n) = model(batch_test)\n",
        "\n",
        "i = np.random.randint(len(pred))\n",
        "plt.title('Prediction example on test set')\n",
        "plt.xlabel('Time')\n",
        "plt.plot(batch_test[i,1:,0].detach().cpu().T)\n",
        "plt.plot(pred[i,:-1,0].detach().cpu().T)\n",
        "plt.show()\n",
        "\n",
        "plt.scatter(batch_test[:, 1:,0].cpu().detach(), pred[:,:-1,0].detach().cpu(), s=5)\n",
        "plt.plot([batch_test.min().item(), batch_test.max().item()], \n",
        "         [batch_test.min().item(), batch_test.max().item()], color='red',)\n",
        "plt.ylabel('Test predictions')\n",
        "plt.xlabel('Test targets')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIynxP_eVpvT"
      },
      "source": [
        "#### Applications:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKaUD5_B0pE5"
      },
      "source": [
        "- forecasting\n",
        "- anomaly detection\n",
        "- encoding latent representation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IpTjHMxv0hz9"
      },
      "source": [
        "#### Improvements and ideas to explore together:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAJkcG6n0vxG"
      },
      "source": [
        "- loss\n",
        "- window predictions\n",
        "- dropout\n",
        "- visualisation of latent representation with TSNE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-gCSmB50wrN"
      },
      "source": [
        "### Imputing LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZShPHfUfKRqK"
      },
      "source": [
        "class LSTMI(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size=None, num_layers=1, dropout=0.):\n",
        "        \"\"\"Define an LSTM Imputer network\n",
        "\n",
        "\n",
        "        Args:\n",
        "          input_size: dimensionality of the input sequences\n",
        "          hidden_size: Number of units for the LSTM cells\n",
        "          output_size: dimensionality of the output sequences.\n",
        "                       If default (None) will be set as input_size.\n",
        "          num_layers: number of LSTM layers\n",
        "          dropout: dropout rate to apply after all-but-last layers\n",
        "        Returns:\n",
        "          pytorch module \n",
        "\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size if output_size is not None else input_size\n",
        "        self.num_layers = num_layers\n",
        "        self.dropout = float(dropout)\n",
        "\n",
        "        self.lstm_cells = nn.ModuleList([nn.LSTMCell(input_size=self.input_size, hidden_size=self.hidden_size)])\n",
        "        self.lstm_cells.extend([nn.LSTMCell(input_size=self.hidden_size, hidden_size=self.hidden_size)\n",
        "                                for _ in range(self.num_layers - 1)])\n",
        "        if dropout != 0:\n",
        "            self.dropout_layer = nn.Dropout(self.dropout)\n",
        "\n",
        "        self.fc = nn.Linear(self.hidden_size, self.output_size)\n",
        "\n",
        "        self.h_t = None\n",
        "        self.c_t = None\n",
        "        self.out_t = None\n",
        "\n",
        "    def init_state(self, batch_size, device=None, dtype=None):\n",
        "      \"\"\"Initialise the network's states\"\"\"\n",
        "      self.h_t = [torch.randn(batch_size, self.hidden_size, device=device, dtype=dtype)\n",
        "                  for _ in range(self.num_layers)]\n",
        "      self.c_t = [torch.randn(batch_size, self.hidden_size, device=device, dtype=dtype)\n",
        "                  for _ in range(self.num_layers)]\n",
        "      self.out_t = torch.randn(batch_size, self.output_size, device=device, dtype=dtype)\n",
        "\n",
        "    def init_state_like(self, x):\n",
        "      \"\"\"Initialise the network's on the model of a given input\"\"\"\n",
        "      self.init_state(len(x), x.device, x.dtype)\n",
        "\n",
        "    def forward(self, x, z=None, m=None, mask_nans=True):\n",
        "      \"\"\"Performs a forward pass\n",
        "\n",
        "      :param x: Input vector\n",
        "      :param m: Imputation mask - 1/True for keeping input and 0/False for forcing dynamic imputation\n",
        "      :return: Output vector with imputed values\n",
        "      \"\"\"\n",
        "\n",
        "      # Checking - casting inputs\n",
        "      if m is not None:\n",
        "        if x.shape != m.shape:\n",
        "          mess = f'x and m must have the same shape ({x.shape} != {m.shape})'\n",
        "          raise RuntimeError(mess)\n",
        "      else:\n",
        "        m = torch.ones_like(x)\n",
        "\n",
        "      if mask_nans:\n",
        "        m *= (x == x)\n",
        "\n",
        "      if len(x.shape) == 2:\n",
        "        warnings.warn('input has only 2 dims')\n",
        "        x = x.unsqueeze(-1)\n",
        "        m = m.unsqueeze(-1)\n",
        "      elif len(x.shape) != 3:\n",
        "        print(x.shape)\n",
        "        raise ValueError('wrong shape')\n",
        "\n",
        "      batch_size, len_seq, n_dim = x.shape\n",
        "\n",
        "      if z is not None:\n",
        "        zdim = z.shape[-1]\n",
        "      else:\n",
        "        zdim = 0\n",
        "      if n_dim + zdim != self.input_size:\n",
        "        mess = f'input (dim={n_dim}) and covariate (dim={zdim}) dimensions must' \\\n",
        "                + f'add to input_size param ({self.input_size})'\n",
        "        raise RuntimeError(mess)\n",
        "\n",
        "      m = m.type(x.dtype)\n",
        "      inverted_m = (torch.ones_like(m, device=x.device) - m)\n",
        "\n",
        "      # Preparing input\n",
        "      if z is not None:\n",
        "        input_masked = torch.cat((x * m, z), dim=-1)\n",
        "      else:\n",
        "        input_masked = x * m\n",
        "      outputs = []\n",
        "\n",
        "      for t in range(len_seq):\n",
        "        pred_t = self.out_t\n",
        "        input_t = input_masked[:, t] + F.pad(pred_t * inverted_m[:, t], (0, zdim))\n",
        "        for k, lstm_cell in enumerate(self.lstm_cells):\n",
        "          if k == 0:\n",
        "            self.h_t[0], self.c_t[0] = lstm_cell(input_t, (self.h_t[0], self.c_t[0]))\n",
        "          else:\n",
        "            self.h_t[k], self.c_t[k] = lstm_cell(self.h_t[k - 1], (self.h_t[k], self.c_t[k]))\n",
        "          if self.dropout != 0 and k < self.num_layers - 1:\n",
        "            self.h_t[k] = self.dropout_layer(self.h_t[k])\n",
        "            self.c_t[k] = self.dropout_layer(self.c_t[k])\n",
        "        self.out_t = self.fc(self.h_t[-1])\n",
        "        outputs.append(self.out_t.unsqueeze(1))\n",
        "      return torch.cat(outputs, 1)\n",
        "\n",
        "def train_lstmi_batch(model, loader, optimiser, criterion, epochs=1, device=None):\n",
        "  \"\"\"Train a LSTMI module\"\"\"\n",
        "  model.train()\n",
        "  losses = []\n",
        "\n",
        "  for epoch in tqdm.tqdm(range(1, 1+epochs)):\n",
        "    epoch_loss = 0\n",
        "    for x in loader:\n",
        "      optimiser.zero_grad()\n",
        "      x = x.to(device)\n",
        "      model.init_state_like(x)\n",
        "      with torch.enable_grad():\n",
        "        pred = model(x)\n",
        "        loss = criterion(x[:, 1:], pred[:, :-1])   # x_{t+1} ~ f(x_t)\n",
        "      loss.backward()\n",
        "      optimiser.step()\n",
        "      epoch_loss += loss.item()\n",
        "    losses.append(epoch_loss / len(loader))\n",
        "  return losses  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5eApdMmPlHJ"
      },
      "source": [
        "model = LSTMI(1, hidden_size=hidden_size, num_layers=num_layers).to(device)\n",
        "optimiser = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "criterion = lambda y, pred: F.mse_loss(y, pred)\n",
        "losses = train_lstmi_batch(model, loader, optimiser, criterion, epochs=200, device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7SdrQTegOlo"
      },
      "source": [
        "# learning curve\n",
        "plt.plot(losses)\n",
        "plt.yscale('log')\n",
        "plt.title('learning curve')\n",
        "plt.xlabel('Training loss')\n",
        "plt.ylabel('MSE')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UG750-cGPND5"
      },
      "source": [
        "# Check of predictions bias\n",
        "model.eval()\n",
        "model.init_state_like(batch_test.to(device))\n",
        "pred = model(batch_test.to(device)).cpu()\n",
        "\n",
        "i = np.random.randint(len(pred))\n",
        "plt.plot(batch_test[i,1:,0].detach().cpu().T, label='data')\n",
        "plt.plot(pred[i,:-1,0].detach().T, label='predictions')\n",
        "plt.scatter(np.where((batch_test[i,:-1,0]==0).detach().to('cpu'))[0],\n",
        "            pred[i,:-1,0][batch_test[i,:-1,0]==0].detach(), color='red',\n",
        "            label='imputed')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "plt.scatter(batch_test[:, 1:,0].detach().cpu(), pred[:,:-1,0].detach().cpu(), s=5)\n",
        "plt.plot([batch_test.min().item(), batch_test.max().item()], \n",
        "         [batch_test.min().item(), batch_test.max().item()], color='red')\n",
        "plt.ylabel('Test predictions')\n",
        "plt.xlabel('Test targets')\n",
        "plt.show()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2N6zbrzTT1E"
      },
      "source": [
        "### Applications\n",
        "- Anomaly detections\n",
        "- Imputing\n",
        "- Modelling and fiting gaps\n",
        "- More flexible learning (include in the training loss)\n",
        "\n",
        "\n",
        "### Suggestions:\n",
        "- Gaussian Loss \n",
        "- Gap imputing metric\n",
        "- See improvements with growing dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBaMfYPpmaCh"
      },
      "source": [
        "## II) Embed differentiable physics model in DL framework\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eW4ePNv16VRM"
      },
      "source": [
        "\n",
        "Why hard-coding the physics model in a DL framework? \n",
        "- Computational efficiency, with automatic differenciation and GPU acceleration\n",
        "- Combine with NNs\n",
        "\n",
        "Physics model requirements:\n",
        "- implemented in DL framework (here Pytorch)\n",
        "- vectorised to process batches of samples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f__tXAAzxqJN"
      },
      "source": [
        "DL & Differential Programming\n",
        "- Tensorflow and GradientApe API \n",
        "- Pytorch and autograd \n",
        "- Julia & $\\partial{P}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dP_WuT6YLixJ"
      },
      "source": [
        "Let's define a simple flare model in Pytorch: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ow4-HpV3QyT-"
      },
      "source": [
        "# Define your physics model and dataset class\n",
        "\n",
        "def compute_flare(time, a_0, tau_g, tau_e, t_0=50):\n",
        "  \"\"\"Compute a flare model following https://academic.oup.com/mnras/article/445/3/2268/2907951\n",
        "\n",
        "  Args:\n",
        "    time: tensor\n",
        "    a_0: tensor\n",
        "    tau_g: tensor\n",
        "    tau_e: tensor\n",
        "    t_0: float or tensor\n",
        "  return:\n",
        "    flare intensity as a function of time\n",
        "  \"\"\"\n",
        "  batch_size = len(a_0)\n",
        "  time = time[None,:].repeat(batch_size, 1)\n",
        "  a_0 = a_0.reshape(batch_size,1)\n",
        "  tau_g = tau_g.reshape(batch_size,1)\n",
        "  tau_e = tau_e.reshape(batch_size,1)\n",
        "  if isinstance(t_0, torch.Tensor):\n",
        "    t_0 = t_0.reshape(batch_size,1)\n",
        "\n",
        "  return a_0 * torch.exp(-((time - t_0) * ((time < t_0) / 2*tau_g \n",
        "                                           + (time > t_0) / tau_e))**2)\n",
        "\n",
        "\n",
        "class PhysicsDataset():\n",
        "  \"\"\"torch dataset subclass for simulated physics time series\"\"\"\n",
        "  def __init__(self, physics_model, bounds, target_params, \n",
        "               seq_length=200, size=100, noise_level=1e-4, seed=0):\n",
        "    \"\"\"Create an instance of PhysicsDataset clsas\n",
        "\n",
        "    Args:\n",
        "      physics_model: function\n",
        "        function taking time tensor as first arg and returning a torch time series\n",
        "      bounds: dict\n",
        "        dictionnary of uniform bounds for physics_model params\n",
        "      target_params: list\n",
        "        list of params to produce as outputs\n",
        "      seq_length: int\n",
        "        time series length (default 200)\n",
        "      size: int\n",
        "        size of the dataset (default 100)\n",
        "      noise_level: double\n",
        "        gassian standard deviation for the additive noise (default 1e-4)\n",
        "      seed: int\n",
        "        random seed (default 0)\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    self.physics_model = physics_model\n",
        "    self.bounds = bounds\n",
        "    self.target_params = target_params\n",
        "    self.seq_length = seq_length\n",
        "    self.size = size\n",
        "    self.noise_level = noise_level\n",
        "    if seed is not None:\n",
        "        torch.manual_seed(seed)\n",
        "    \n",
        "    self.params = dict()\n",
        "    self._sample_priors()\n",
        "    self.time = torch.linspace(0, 100, self.seq_length)\n",
        "    self.noise = torch.randn(self.size, self.seq_length) * self.noise_level\n",
        "    self.physics = self.physics_model(self.time, **self.params)\n",
        "\n",
        "  def _sample_priors(self):\n",
        "    \"\"\"Sample parameters inside object's defined bounds with uniform distributions\"\"\"\n",
        "    for par in self.bounds:\n",
        "      self.params[par] = torch.rand(self.size) * (self.bounds[par][1] - self.bounds[par][0]) + self.bounds[par][0]\n",
        "\n",
        "  def __len__(self):\n",
        "      return self.size\n",
        "  \n",
        "  def __getitem__(self, index):\n",
        "    x = self.physics[index] + self.noise[index]\n",
        "    target = torch.tensor([self.params[par][index] for par in self.target_params])\n",
        "    return x, target #, additional_params\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-C8NOnZ4C3Z5"
      },
      "source": [
        "# define example and plot ssamples\n",
        "\n",
        "dataset = PhysicsDataset(compute_flare, \n",
        "                         bounds={'a_0': [1, 5], 'tau_g':[4, 6], 'tau_e':[1, 5]}, \n",
        "                         target_params=['a_0', 'tau_g', 'tau_e'], \n",
        "                         noise_level=0.05)\n",
        "\n",
        "plt.plot(compute_flare(dataset.time, **dataset.params).detach().cpu().T)\n",
        "plt.xlabel('time')\n",
        "pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8jqEboqzzX5"
      },
      "source": [
        "# Instantiating datasets and loaders\n",
        "\n",
        "device = 'cpu'  # cpu can sometimes be faster than cuda\n",
        "\n",
        "# Datasets parameters\n",
        "target_params = ['a_0', 'tau_g', 'tau_e']\n",
        "bounds_train = {'a_0': [1, 2], 'tau_g':[4, 6], 'tau_e':[2, 5]}\n",
        "bounds_val_2 = {'a_0': [0, 3], 'tau_g':[3, 8], 'tau_e':[1, 6]}  # extending the bounds\n",
        "noise_train = 0.01\n",
        "\n",
        "# General params\n",
        "seq_length = 100\n",
        "size_train = 256  # limited data scenario\n",
        "batch_size = 64\n",
        "size_val = 512\n",
        "size_test = 1024\n",
        "\n",
        "# Train dataset\n",
        "dataset_train = PhysicsDataset(compute_flare, bounds_train, target_params, \n",
        "                               seq_length=seq_length, size=size_train, \n",
        "                               noise_level=noise_train, seed=0)\n",
        "\n",
        "loader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Val dataset 1 (same parameter space as training)\n",
        "dataset_val_1 = PhysicsDataset(compute_flare, bounds_train, target_params, \n",
        "                                seq_length=seq_length, size=size_val, \n",
        "                                noise_level=noise_train, seed=1)\n",
        "loader_val_1 = DataLoader(dataset_val_1, batch_size=len(dataset_val_1))\n",
        "batch_val_1 = next(iter(loader_val_1))\n",
        "\n",
        "# Val dataset 2 (larger parameter space)\n",
        "dataset_val_2 = PhysicsDataset(compute_flare, bounds_val_2, target_params, \n",
        "                                seq_length=seq_length, size=size_val, \n",
        "                                noise_level=noise_train, seed=2)\n",
        "loader_val_2 = DataLoader(dataset_val_2, batch_size=len(dataset_val_2))\n",
        "batch_val_2 = next(iter(loader_val_2))\n",
        "\n",
        "# Test dataset 1 (same parameter space as training)\n",
        "dataset_test_1 = PhysicsDataset(compute_flare, bounds_train, target_params, \n",
        "                                seq_length=seq_length, size=size_test, \n",
        "                                noise_level=noise_train, seed=3)\n",
        "loader_test_1 = DataLoader(dataset_test_1, batch_size=len(dataset_test_1))\n",
        "x_test_1, target_test_1 = next(iter(loader_test_1))\n",
        "\n",
        "# Test dataset 2 (larger parameter space)\n",
        "dataset_test_2 = PhysicsDataset(compute_flare, bounds_val_2, target_params, \n",
        "                                seq_length=seq_length, size=size_test, \n",
        "                                noise_level=noise_train, seed=4)\n",
        "\n",
        "loader_test_2 = DataLoader(dataset_test_2, batch_size=len(dataset_test_2))\n",
        "x_test_2, target_test_2 = next(iter(loader_test_2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7fJEkWKVKmV"
      },
      "source": [
        "Let's define a simple MLP network to solve the inverse problem:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGA85RQ9IxTa"
      },
      "source": [
        "# A simple network to solve the inverse problem data -> params\n",
        "\n",
        "class Network(nn.Module):\n",
        "  \"\"\"Define a simple MLP in pytorch\"\"\"\n",
        "  def __init__(self, seq_length, out_dim):\n",
        "    super().__init__()\n",
        "    self.fc1 = nn.Linear(seq_length, 512)\n",
        "    self.fc2 = nn.Linear(512, 128)\n",
        "    self.fc3 = nn.Linear(128, 16)\n",
        "    self.fc4 = nn.Linear(16, out_dim)\n",
        "      \n",
        "  def forward(self, x):\n",
        "    out = F.relu(self.fc1(x))\n",
        "    out = F.relu(self.fc2(out))\n",
        "    out = F.relu(self.fc3(out))\n",
        "    out = self.fc4(out)\n",
        "    return out\n",
        "\n",
        "\n",
        "def train_network(model, loader, optimiser, criterion,\n",
        "                  epochs=1, batch_val={}, metric_val=None, eval_epochs=[]):\n",
        "  \"\"\"Train a pytorch module\"\"\"\n",
        "  losses = {'train':[]}\n",
        "  losses.update({name: [] for name in batch_val})\n",
        "\n",
        "  for epoch in tqdm.tqdm(range(1, 1+epochs)):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    for x, target in loader:\n",
        "      optimiser.zero_grad()\n",
        "      x = x.to(device)\n",
        "      target = target.to(device)\n",
        "      with torch.enable_grad():\n",
        "        pred = model(x)\n",
        "        loss = criterion(x, target, pred)\n",
        "      loss.backward()\n",
        "      optimiser.step()\n",
        "      epoch_loss += loss.item()\n",
        "    losses['train'].append(epoch_loss / len(loader))\n",
        "    if metric_val is not None and batch_val is not None and epoch in eval_epochs:\n",
        "      model.eval()\n",
        "      for name, batch in batch_val.items():\n",
        "        x_val, target_val = batch[0].to(device), batch[1].to(device)\n",
        "        pred_val = model(x_val)\n",
        "        losses[name].append(metric_val(x_val, target_val, pred_val))\n",
        "  return losses  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cr2p1DC05lfJ"
      },
      "source": [
        "# Define two different losses\n",
        "\n",
        "# Classic MSE regression loss\n",
        "def naive_loss(x, target, pred):\n",
        "  \"\"\"Wrapper around pytorch mse_loss to add inputs to signature\n",
        "\n",
        "  Args: \n",
        "    x: torch.Tensor \n",
        "      input time series of shape (batch_size, T) or (T,). (unused argument)\n",
        "    target: torch.Tensor\n",
        "      output targets of shape (batch_size, dim) or (dim,)\n",
        "    pred: torch.Tensor\n",
        "      predicted targets of shape (batch_size, dim) or (dim,)\n",
        "  Return: torch.Tensor\n",
        "    mean squared error value between target and predictions\n",
        "  \"\"\"\n",
        "  return F.mse_loss(target, pred)\n",
        "\n",
        "# Hybrid regression and physics reconstruction\n",
        "def hybrid_loss(dataset, beta=1):\n",
        "  \"\"\"Define a hybrid regression & reconstruction loss function\n",
        "\n",
        "  Args:\n",
        "    dataset: PhysicsDataset\n",
        "      torch dataset with arguments target_params, time and physics_model\n",
        "    beta: \n",
        "      weight parameter between regression and reconstruction terms: \n",
        "      loss = regression + beta * reconstruction\n",
        "  Return: function\n",
        "    loss function associated with provided dataset and beta parameter\n",
        "  \"\"\"\n",
        "  def loss_function(x, target, pred):\n",
        "    \"\"\"Compute the hybrid loss\n",
        "\n",
        "    Args: \n",
        "    x: torch.Tensor \n",
        "      input time series of shape (batch_size, T) or (T,). (unused argument)\n",
        "    target: torch.Tensor\n",
        "      output targets of shape (batch_size, dim) or (dim,)\n",
        "    pred: torch.Tensor\n",
        "      predicted targets of shape (batch_size, dim) or (dim,)\n",
        "    Return: torch.Tensor\n",
        "     mean squared error value between target and predictions added to mean squared error \n",
        "    \"\"\"\n",
        "    pred_dict = {dataset.target_params[i]: pred[:,i] for i in range(len(dataset.target_params))}  # Iterate over feature dimension to produce dict of outputs\n",
        "    reconstructed_time_series = dataset.physics_model(dataset.time.to(device), **pred_dict)\n",
        "    physics_reconstruction_term = F.mse_loss(x, reconstructed_time_series)\n",
        "    return naive_loss(x, target, pred) + beta * physics_reconstruction_term\n",
        "  return loss_function\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEOmc0iNVlcL"
      },
      "source": [
        "# Create 2 scenarios with identical networks but different losses\n",
        "scenarios = ['naive', 'hybrid']\n",
        "network = {scenario: Network(seq_length, len(target_params)).to(device) for scenario in scenarios}\n",
        "optimiser = {scenario: torch.optim.Adam(network[scenario].parameters(), lr=0.001) for scenario in scenarios}\n",
        "loss = {'naive': naive_loss,\n",
        "        'hybrid': hybrid_loss(dataset_train,beta=0.1)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gcPOai6_eiQA"
      },
      "source": [
        "# Running experiment for two scenarios\n",
        "loss_history = dict()\n",
        "epochs = 2000\n",
        "eval_epochs = list(range(1, 1+epochs, 10))\n",
        "for scenario in scenarios:\n",
        "  loss_history[scenario] = train_network(network[scenario], loader_train, optimiser[scenario], loss[scenario], epochs=epochs, \n",
        "                                         batch_val={'val_1': batch_val_1, 'val_2': batch_val_2}, metric_val=naive_loss, eval_epochs=eval_epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CnmlF_LWF6RY"
      },
      "source": [
        "# Validation loss curve n°1\n",
        "\n",
        "linestyle = {'naive': 'solid', 'hybrid': 'dotted'}\n",
        "alpha = {'naive': 0.7, 'hybrid': 1}\n",
        "for scenario in scenarios:\n",
        "  plt.plot(eval_epochs, loss_history[scenario]['val_1'], label=f'Val 1 ({scenario})', \n",
        "           c='blue', linestyle=linestyle[scenario], alpha=alpha[scenario])\n",
        "plt.legend()\n",
        "plt.yscale('log')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('MSE')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGPWXIQb2wf8"
      },
      "source": [
        "# Validation loss curve n°2\n",
        "\n",
        "for scenario in scenarios:\n",
        "\n",
        "  plt.plot(eval_epochs, loss_history[scenario]['val_2'], label=f'Val 2 ({scenario})', \n",
        "           c='green', linestyle=linestyle[scenario], alpha=alpha[scenario])\n",
        "plt.legend()\n",
        "plt.yscale('log')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('MSE')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7oTJj9Lbgqe"
      },
      "source": [
        "# Evaluation on test sets\n",
        "\n",
        "scores_1 = dict()\n",
        "scores_2 = dict()\n",
        "\n",
        "for scenario in scenarios:\n",
        "  scores_1[scenario] = naive_loss(x_test_1, network[scenario](x_test_1), target_test_1).item()\n",
        "  scores_2[scenario] = naive_loss(x_test_2, network[scenario](x_test_2), target_test_2).item()\n",
        "print(f'scores_1: {scores_1}')\n",
        "print(f'scores_2: {scores_2}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5pbx43yZvxa"
      },
      "source": [
        "Pros:\n",
        "- accuracy\n",
        "- stability\n",
        "- generalisability\n",
        "\n",
        "Cons:\n",
        "- need the physics model implemented in DL framework\n",
        "- physics model suceptible of adding complexity to the network's! \n",
        "- training may require further tuning to accomodate for different loss terms\n",
        "\n",
        "Improvements:\n",
        "- hyperoptimisation for the loss weight $\\beta$ \n",
        "- hyperoptim for LRs in both cases\n",
        "\n",
        "To go further:\n",
        "- use your own physics model! \n",
        "- design transfer learning and meta-learning experiments to assess generalisability\n",
        "- Combine LSTM detrending simultaneously with differentiable physics model\n",
        "- Gaussian constraints on param space with VAE\n",
        "- SVI for gaussian inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2MbmzBjTu6M"
      },
      "source": [
        "### Different examples of hybrid architectures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRqbTsoQUiQL"
      },
      "source": [
        "Example in physics of exoplanetary transits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EeMr4DtOq7sc"
      },
      "source": [
        "![alt text](https://raw.githubusercontent.com/mariomorvan/nam21-astro-ts-physics-dl/main/hybrid_architectures_transit_NN.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDBPACXWT0rh"
      },
      "source": [
        "Figure from [*PyLightcurve-torch: a transit modelling package for deep learning applications in PyTorch*](https://arxiv.org/abs/2011.02030)"
      ]
    }
  ]
}